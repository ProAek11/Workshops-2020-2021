{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Working with pandas in Python! BLANK.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nnn_61HWMTS9"
      },
      "source": [
        "<h1><center><span style=\"color:purple\">Working with pandas!</center></h1>\n",
        "\n",
        "![Pandas for Data Analysis and their Benefits | by Asirinaidu P | Medium](https://miro.medium.com/max/748/1*wP8ubuQEIrtxtfd-DTOTig.jpeg)\n",
        "\n",
        "<h1><center><span style=\"color:purple\">pandas pandas pandas</center></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdIBlxIiMTS_"
      },
      "source": [
        "<h1><center><span style=\"color:purple\">A Background on pandas</center></h1>\n",
        "\n",
        "**pandas was first developed by Wes McKinney in 2008 while he worked for a hedge fund. Wes saw how cumbersome data analysis was in programs such as Excel and SQL, so he began developing tools for data analysis which lead to the development of pandas. When developing pandas, Wes wanted to help people make the transition from Excel to pandas as smooth as possible, so he stressed usability when developing pandas and made sure to include a suite of features. By the end of 2009, pandas had been open sourced, and is actively supported today by a community of like-minded individuals around the world.**\n",
        "\n",
        "![The rise in popularity of Pandas](https://s3.us-east-1.amazonaws.com/qz-production-atlas-assets/charts/atlas_rJ9sZ5syf@2x.png)\n",
        "\n",
        "**The name pandas is a reference to panel data. For those that have worked with Excel and csv files, those are examples of panel data.**\n",
        "\n",
        "**pandas is closely tied to the NumPy module. NumPy arrays can easily be created from pandas DataFrames and vice versa. Examples will be shown in the code below.**\n",
        "\n",
        "**Data scientists across various industries and companies including Postmates, Spotify, JP Morgan and Tesla use the pandas module.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7-0JuxRMTTC"
      },
      "source": [
        "<h1><center><span style=\"color:purple\">Goal of the Workshop</center></h1>\n",
        "\n",
        "**The goal is to give an introduction of the pandas module within Python. This will be done by using pandas to conduct Exploratory Data Analysis (EDA) on a data set. The data set that will be used is a collection of fitness data composed of daily run statistics for an individual. There are other files that have been included within the folder and we encourage everyone to play with these data sets on their own. Let's get started!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1wbiuCOMTTE"
      },
      "source": [
        "#import the pandas module, if you do not have it install the module before the session begins\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29Y5uIH1MTTO",
        "outputId": "f7f160ca-320e-4149-8a89-59b88b78a475",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''pandas makes it easy to read in a variety of files including: csv, Excel, JSON, ORC and many other file types\n",
        "The data set we are working with today is stored within a csv file. pandas has a function that reads the file and\n",
        "can store it within a DataFrame. A DataFrame is a 2-dimensional labeled data structure with columns of potentially \n",
        "different types. DataFrames are similar aesthetically to an Excel Spreadsheet. We will now read in the data into a\n",
        "DataFrame and show the DataFrame within the notebook.\n",
        "'''\n",
        "\n",
        "\n",
        "'''The DataFrame is illustrated below. There are columns for each of the categories and the rows contain the data for the\n",
        "columns. There is also an index, which is similar to the index you see for lists, where index position 0 is the first\n",
        "row of data points'''\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The DataFrame is illustrated below. There are columns for each of the categories and the rows contain the data for the\\ncolumns. There is also an index, which is similar to the index you see for lists, where index position 0 is the first\\nrow of data points'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1Gz1UGxMTTV"
      },
      "source": [
        "# **Now that we have read the file, we can take a look at the data. The DataFrame lays everything out in an easy to read way. We can see the date of each run, the distance covered, how long the run lasted and the calories burned from the run.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY7RkBuWMTTX",
        "outputId": "28f47a42-3598-4fe9-93b6-543c19e58756",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''Let's take a look at the data types of each column. The dtypes function lists this out for us. We can see that there\n",
        "the Activity ID is comprised of integers, Distance in Kilometers is comprised of floats and the Activity Name is comprised\n",
        "of objects. Usually an object is a Python string, but these columns can also refer to mixed data types.'''\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Let's take a look at the data types of each column. The dtypes function lists this out for us. We can see that there\\nthe Activity ID is comprised of integers, Distance in Kilometers is comprised of floats and the Activity Name is comprised\\nof objects. Usually an object is a Python string, but these columns can also refer to mixed data types.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKimnW59MTTe",
        "outputId": "ca6400f8-718a-4cf9-f496-affda00a5b97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "'''Let's work to clean up the data. One of the first steps we can take is to get rid of columns that we \n",
        "won't use for our EDA. Columns like Activity ID won't be necessary for us to have because they are IDs used within \n",
        "the fitness app and our arbitrary. We can get rid of the column by using the drop function. We specify the column\n",
        "name and set inplace to True to drop the column permanently'''\n",
        "\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Let's work to clean up the data. One of the first steps we can take is to get rid of columns that we \\nwon't use for our EDA. Columns like Activity ID won't be necessary for us to have because they are IDs used within \\nthe fitness app and our arbitrary. We can get rid of the column by using the drop function. We specify the column\\nname and set inplace to True to drop the column permanently\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03KZtcUiMTTj"
      },
      "source": [
        "**The next step we can take is to work with null values within the DataFrame. Null values are missing values within the\n",
        "data set which can give us trouble in our analysis. We need to deal with these null values be either deleting the row or\n",
        "column that contains the null values, or imputing a value into the empty row. Imputing a value means adding an estimated\n",
        "value in for the null value. For example if we were missing a row for the Distance in Kilometers column, we could impute\n",
        "a value by usinge the mean distance run.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "on2fICIRMTTk",
        "outputId": "c37adede-d7b3-4804-d118-c75d070e169e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "'''First let's take a look to see where we have null values within the data set. We know that there are 228 rows of data\n",
        "within our DataFrame. The isnull function returns all rows of data that contain a null value within the column and we can\n",
        "use the sum function to get the total amount of null values within the column. Columns such as Activity Description and \n",
        "Relative Effort can be dropped since all the values in the column are null values'''\n",
        "\n",
        "\n",
        "'''We can get rid of these null columns by using the dropna function which can get rid of columns with null values. We specify\n",
        "that we are dropping columns within the DataFrame. We can also input a Threshold for dropping columns. For example, if we set\n",
        "the threshold to 114, then we are saying that a column needs values for at least half the data, or else it will be dropped'''\n",
        "\n",
        "\n",
        "'''To filter for just running activities, we can use the loc function. loc enables us to hone in on a specific variable and\n",
        "edit the rows and columns based on an arguement that we pass it. In this case, under the Activity Type column we can specify\n",
        "that we want to return rows that are runs, not rides (which means biking). We can then set the frame equal to save this\n",
        "modification.'''\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'To filter for just running activities, we can use the loc function. loc enables us to hone in on a specific variable and\\nedit the rows and columns based on an arguement that we pass it. In this case, under the Activity Type column we can specify\\nthat we want to return rows that are runs, not rides (which means biking). We can then set the frame equal to save this\\nmodification.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsQm1jfKMTTr",
        "outputId": "ae4f6cab-4792-45bf-d3e3-bd516331f3ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "'''Finally, we have 4 columns that are missing one data value each. It would be unwise to get rid of a column of data if\n",
        "less than 1% of the data is missing. This is where we can impute a value. A standard way of imputing data is to fill the\n",
        "null value with the mean of the available data.'''\n",
        "\n",
        "'''df['Max Speed'].fillna(df['Max Speed'].mean(), inplace = True)\n",
        "df['Elevation Gain'].fillna(df['Elevation Gain'].mean(), inplace = True)\n",
        "df['Elevation Loss'].fillna(df['Elevation Loss'].mean(), inplace = True)\n",
        "df['Elevation Low'].fillna(df['Elevation Low'].mean(), inplace = True)\n",
        "df['Elevation High'].fillna(df['Elevation High'].mean(), inplace = True)\n",
        "df['Calories'].fillna(df['Calories'].mean(), inplace = True)'''\n",
        "\n",
        "#A more pythonic way for people more familiar with the language is below, this can be run instead of the above code\n",
        "\n",
        "'''for i,j in list(zip(df.isnull().sum().index,df.isnull().sum())):\n",
        "    if j > 0:\n",
        "        df[i].fillna((df[i].mean()), inplace = True)'''"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'for i,j in list(zip(df.isnull().sum().index,df.isnull().sum())):\\n    if j > 0:\\n        df[i].fillna((df[i].mean()), inplace = True)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEwTv0teMTTw"
      },
      "source": [
        "#A look at the edited DataFrame\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnfHK9CQMTT0",
        "outputId": "6e7617c4-7175-46ca-8a2c-6306a781e99c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''If we want to conduct a time series analysis with our data, converting the data to datetime is the first step.\n",
        "When a column with dates or strings is initially uploaded to a DataFrame, the data type will be an object. We can easily\n",
        "convert to datetime type by using the to_datetime function.'''\n",
        "\n",
        "\n",
        "#We can check that the data was changed from object to datetime calling the specific column. The data type is listed at the bottom\n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'If we want to conduct a time series analysis with our data, converting the data to datetime is the first step.\\nWhen a column with dates or strings is initially uploaded to a DataFrame, the data type will be an object. We can easily\\nconvert to datetime type by using the to_datetime function.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsrA-dPeMTT4"
      },
      "source": [
        "**Now that we have our data cleaned up, we want to get a better idea of the high level statistics about the data. We also decide that we want to focus on calories, specifically what other variables will affect the calories burned. We also want to\n",
        "focus on the calories that are burned from running.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPA2rcQvMTT5",
        "outputId": "46763db6-db65-4fc5-fc37-2df71439f78a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "'''Lets say that we are used to measuring distance in miles rather than in kilometers. We can add a column to the DataFrame\n",
        "that lists the runs in miles. To do this, we take the DataFrame and put a bracket around the name of the column we want to\n",
        "create. We set the new column equal to the kilometer values multiplied by the approximate number to transform the values to\n",
        "miles. The new column will be placed at the end of the DataFrame'''\n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Lets say that we are used to measuring distance in miles rather than in kilometers. We can add a column to the DataFrame\\nthat lists the runs in miles. To do this, we take the DataFrame and put a bracket around the name of the column we want to\\ncreate. We set the new column equal to the kilometer values multiplied by the approximate number to transform the values to\\nmiles. The new column will be placed at the end of the DataFrame'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmzG5MtEMTT-"
      },
      "source": [
        "#The describe function outputs some summary statistics for each column. This includes the number of observations for each\n",
        "#variable, the mean, standard deviation, minimum, maximum and percentiles\n",
        "\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AybWmCpOMTUD"
      },
      "source": [
        ""
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4kyBPV0MTUH"
      },
      "source": [
        "**Let's focus on just the calories column. Perhaps the most interesting feature is the variability of the Calories burned.\n",
        "Standard devation can tell us how volatile the data are. For calories burned, if there is a high standard devation, then the\n",
        "chances of seeing a large amount of calories burned in one run then a small amount of calories burned in another run is higher. We can see that the standard deviation is relatively high. This is further illustrated by the minimum and maximum values for calories burned. The minimum burned for one run was only 60 calories, while the maximum burned was a whopping 1,392!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDL6UNO2MTUI",
        "outputId": "dd4edc80-822b-4f21-c6d9-1db540c619fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "'''Next we can plot a histogram of the calorie observations. Bars are divided by the bin number. The range of the bars are\n",
        "incremented by 50, the value of the bin. We can see see that a large portion of the calories burned are betwee 200-300 per run. \n",
        "We can also see how spread out the data is, which is an illustration of the high standard deviation. Also remeber\n",
        "to include a semicolon at the end of any plot you have created. This is a remnant from programs like MATLAB where this stops\n",
        "code from being printed along with the graph.'''\n",
        "\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Next we can plot a histogram of the calorie observations. Bars are divided by the bin number. The range of the bars are\\nincremented by 50, the value of the bin. We can see see that a large portion of the calories burned are betwee 200-300 per run. \\nWe can also see how spread out the data is, which is an illustration of the high standard deviation. Also remeber\\nto include a semicolon at the end of any plot you have created. This is a remnant from programs like MATLAB where this stops\\ncode from being printed along with the graph.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ne50VYCGMTUO",
        "outputId": "6e03d497-9e20-46ff-f74e-650b98a70298",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "'''Next we can graph the calories burned over time to see if there is any visual pattern that we can spot for seasonality. \n",
        "We'll use Matplotlib's barchart to graph the data. To input the values to the chart, we need to pass in a list or \n",
        "NumPy array into the plt function. Selecting the DataFrame then using the dot values function will output a NumPy array of \n",
        "the data which is used to create the chart. Visually, there may be a pattern where the frequency of runs increase in the summer\n",
        "time, around June to August, but we do not have enough observations to make such a conclusion.'''\n",
        "\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Next we can graph the calories burned over time to see if there is any visual pattern that we can spot for seasonality. \\nWe'll use Matplotlib's barchart to graph the data. To input the values to the chart, we need to pass in a list or \\nNumPy array into the plt function. Selecting the DataFrame then using the dot values function will output a NumPy array of \\nthe data which is used to create the chart. Visually, there may be a pattern where the frequency of runs increase in the summer\\ntime, around June to August, but we do not have enough observations to make such a conclusion.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsOiHHJxMTUT",
        "outputId": "ba72b014-5d5f-4128-d767-8dcc642fc147",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''Say that we want to look at a time series of runs on a monthly basis, instead of a daily basis. To do this we can use the\n",
        "resample function. First we set our Activity Date column to the index. You will now notice that if you run the DataFrame\n",
        "instead of the \"0, 1, 2\" values for the index, we now have the date values as the index. Next we call the resample function.\n",
        "We set it to one month and sum all the values within the month.'''\n",
        "\n",
        "\n",
        "\n",
        "'''Now we have a sum of all the columns. However, a few of the columns do not lend themselves to being summed. For instance\n",
        "Max Speed cannot be summed, it's an average. Let's focus on just a few columns including: Elapsed Time, Distance and Calories.\n",
        "We can modify the DataFrame to only hold those columns by using the below code.'''\n",
        "\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Now we have a sum of all the columns. However, a few of the columns do not lend themselves to being summed. For instance\\nMax Speed cannot be summed, it's an average. Let's focus on just a few columns including: Elapsed Time, Distance and Calories.\\nWe can modify the DataFrame to only hold those columns by using the below code.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weqFQo6IMTUW"
      },
      "source": [
        "#Now let's graph out the calories again to look at how many are burned in total during each month. Again, there may be a \n",
        "#seasonal pattern, but our data set is to small to infer anthing\n",
        "\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAHQFfbdMTUa"
      },
      "source": [
        "**To finish off our EDA, we will look at which variables have a linear correlation to Calories burned and which do not. For\n",
        "anyone that wants to dig into the subject matter futher, [here is a link](https://www.khanacademy.org/math/statistics-probability/describing-relationships-quantitative-data/introduction-to-trend-lines/v/fitting-a-line-to-data) to a series of Khan Academy videos on linear regression. pandas has a function corr which returns pairwise correlations when given a dependent variable. In this case, the dependent variable will be calories and the independent variables will be the rest of the variables.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4vgBpXJMTUc",
        "outputId": "698af0c4-1ec8-4b1c-d8c0-9cbbb2501897",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "'''The output we get is very intuitive. The correlation between the distances run and calories burned is positive and almost\n",
        "perfect. We can interpret this as the further the individual run, the more calories will be burned. Similarly, moving time is\n",
        "also postively correlated to calories burned and is almost perfect. Again, this is very intuitive, the longer the runner spends\n",
        "time moving during the run, we can expect that more calories will be burned. At the top of the list, the correlations are less\n",
        "significant, meaning they may not be a great predictor of calories that will be burned. Keep in mind that this is just a simple\n",
        "linear correlation, there may be non-linear correlations between calories burned and the variables that we do now know of.\n",
        "Finally remember the saying, \"correlation is not causation\". We may have a good idea of what will burn calories, \n",
        "but we can never be totally certain.'''\n",
        "\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The output we get is very intuitive. The correlation between the distances run and calories burned is positive and almost\\nperfect. We can interpret this as the further the individual run, the more calories will be burned. Similarly, moving time is\\nalso postively correlated to calories burned and is almost perfect. Again, this is very intuitive, the longer the runner spends\\ntime moving during the run, we can expect that more calories will be burned. At the top of the list, the correlations are less\\nsignificant, meaning they may not be a great predictor of calories that will be burned. Keep in mind that this is just a simple\\nlinear correlation, there may be non-linear correlations between calories burned and the variables that we do now know of.\\nFinally remember the saying, \"correlation is not causation\". We may have a good idea of what will burn calories, \\nbut we can never be totally certain.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIglCPnsMTUf",
        "outputId": "4bb045a1-c419-41da-ddb4-e3789c803de2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "'''Finally, we will plot out the relationships between the variables. One of the best ways to graph the relationships is with\n",
        "the Seaborn pairplot function. This returns scatter plots for all the variables. The kind parameter allows us to fit \n",
        "linear regression models to the scatter plots. If we take a look at calories which is the second row from the bottom, we can\n",
        "see the relationships for each of the variables. Diagonally, histograms have been plotted to show the distribution of each of\n",
        "the variables.'''\n",
        "\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Finally, we will plot out the relationships between the variables. One of the best ways to graph the relationships is with\\nthe Seaborn pairplot function. This returns scatter plots for all the variables. The kind parameter allows us to fit \\nlinear regression models to the scatter plots. If we take a look at calories which is the second row from the bottom, we can\\nsee the relationships for each of the variables. Diagonally, histograms have been plotted to show the distribution of each of\\nthe variables.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfWy13efMTUj"
      },
      "source": [
        "#We will save our cleaned data frame to a csv file\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mghjRkSLMTUm"
      },
      "source": [
        "# <span style=\"color:purple\">Additional Data Sets\n",
        "\n",
        "**Also included within the folder are additional data sets.**\n",
        "\n",
        "- **The Titanic file contains a well known data set on Titanic passengers. The data set is used for an intoduction to data analysis and machine learning on Kaggle. I highly encourage everyone to check out the site, you can follow the [link here!](https://www.kaggle.com/c/titanic)**\n",
        "<pre></pre>\n",
        "- **The Video Game Sales file contains information on the total sales for individual games such as Super Mario Bros. and Call of Duty: Modern Warfare. It's a relatively simple data set, but it's fun to explore with pandas! A link to the data set online can be found [here](https://data.world/julienf/video-games-global-sales-in-volume-1983-2017)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHZF5C0mMTUn"
      },
      "source": [
        "# <span style=\"color:purple\">Resources to learn more about pandas!\n",
        "\n",
        "## <span style=\"color:blue\">Documentation\n",
        "[pandas Documentation Site](https://pandas.pydata.org/pandas-docs/stable/index.html)\n",
        "\n",
        "## <span style=\"color:blue\">Podcasts\n",
        "1. [Real Python Podcast – Episode 16 – Thinking in Pandas: Python Data Analysis the Right Way](https://www.youtube.com/watch?v=NOBn8r5t5A4) (Optimizing pandas)\n",
        "\n",
        "2. [Podcast._ _init_ _ - Wes McKinney's Career In Python For Data Analysis - Episode 203\n",
        "](https://www.pythonpodcast.com/wes-mckinney-python-for-data-analysis-episode-203/)(Interview with the creator of pandas, Wes McKinney)\n",
        "\n",
        "## <span style=\"color:blue\">Videos\n",
        "1. [Code Basics - Introductory Tutorials to pandas](https://www.youtube.com/watch?v=CmorAWRsCAw&list=PLeo1K3hjS3uuASpe-1LjfG5f14Bnozjwy) (Basic pandas functions including how to read csv files to a DataFrame, using functions such as \"concat\" and \"dropna\" and time series analysis)\n",
        "\n",
        "2. [Bryan Cafferky Python Playlists](https://www.youtube.com/c/BryanCafferky/playlists)    (Covers using Python and pandas for data science. More advanced topics include calling SQL within pandas and working with Databricks)\n",
        "\n",
        "## <span style=\"color:blue\">Articles\n",
        "    \n",
        "   1. [Apache Arrow and the \"10 Things I Hate About pandas\"](https://wesmckinney.com/blog/apache-arrow-pandas-internals/)(Article penned by pandas creator We McKinney on what improvements should be tackled within pandas, including scalability issues, dealing with missing data and supporting categorical data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BULoPbX7MTUn"
      },
      "source": [
        ""
      ],
      "execution_count": 35,
      "outputs": []
    }
  ]
}